
@article{xu_empirical_2015,
	title = {Empirical {Evaluation} of {Rectified} {Activations} in {Convolutional} {Network}},
	url = {http://arxiv.org/abs/1505.00853},
	abstract = {In this paper we investigate the performance of diﬀerent types of rectiﬁed activation functions in convolutional neural network: standard rectiﬁed linear unit (ReLU), leaky rectiﬁed linear unit (Leaky ReLU), parametric rectiﬁed linear unit (PReLU) and a new randomized leaky rectiﬁed linear units (RReLU). We evaluate these activation function on standard image classiﬁcation task. Our experiments suggest that incorporating a nonzero slope for negative part in rectiﬁed activation units could consistently improve the results. Thus our ﬁndings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overﬁtting. They are not as eﬀective as using their randomized counterpart. By using RReLU, we achieved 75.68\% accuracy on CIFAR-100 test set without multiple test or ensemble.},
	language = {en},
	urldate = {2021-06-19},
	journal = {arXiv:1505.00853 [cs, stat]},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	month = nov,
	year = {2015},
	note = {arXiv: 1505.00853},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in C.pdf:/home/notchla/Zotero/storage/FS6AJX33/Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in C.pdf:application/pdf},
}
